{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7126687,"sourceType":"datasetVersion","datasetId":4111193},{"sourceId":7129548,"sourceType":"datasetVersion","datasetId":4113242},{"sourceId":7129644,"sourceType":"datasetVersion","datasetId":4113313}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Import section**","metadata":{"id":"yZzlmsfQPLE6"}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport warnings\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom xml.etree import ElementTree as et\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.ssd import SSDClassificationHead\nfrom torch.optim import lr_scheduler\nfrom torchvision.ops import nms\nfrom collections import Counter\nfrom torchvision import transforms\nimport torch.optim as optim\nfrom torchvision.ops import nms\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:48:31.334366Z","iopub.execute_input":"2023-12-05T23:48:31.334659Z","iopub.status.idle":"2023-12-05T23:48:36.279181Z","shell.execute_reply.started":"2023-12-05T23:48:31.334623Z","shell.execute_reply":"2023-12-05T23:48:36.278214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining image size**","metadata":{}},{"cell_type":"code","source":"size = 720","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:48:40.369993Z","iopub.execute_input":"2023-12-05T23:48:40.370505Z","iopub.status.idle":"2023-12-05T23:48:40.374984Z","shell.execute_reply.started":"2023-12-05T23:48:40.370474Z","shell.execute_reply":"2023-12-05T23:48:40.373841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataset class**","metadata":{}},{"cell_type":"code","source":"class DataSet(Dataset):\n    \n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((size, size)),\n        transforms.ToTensor(),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n        transforms.RandomGrayscale(p=0.1),\n        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225]),\n    ])\n    \n    def __init__(self, root, transforms=None):\n        self.root = root # Root folder (annotation)\n        self.labels = ['bg', 'car'] # All dataset's labels\n        self.l2i = { label: i for i, label in enumerate(self.labels)} # Forward \"Encoder\"\n        self.imgs = sorted([root + '/images/' + i for i in os.listdir(root + '/images')]) # Image paths\n        self.xmls = sorted([root + '/annotation/' + i for i in os.listdir(root + '/annotation')]) # XML paths\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, index):\n        img_path = self.imgs[index]\n        xml_path = self.xmls[index]\n\n        # Image augmentations\n        \n        image = cv2.imread(img_path)\n        H, W, _ = image.shape\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        transformed_image = self.transform(image)\n        \n        # Parsing XML file\n        tree = et.parse(xml_path)\n        root = tree.getroot()\n        boxes = []\n        labels = []\n            \n        for item in root.findall('.//shapes/item'):\n            label = item.find('label').text\n            points = item.find('points')\n            xmin, ymin = [float(point.text) for point in points[0]]\n            xmax, ymax = [float(point.text) for point in points[1]]\n\n            xmin, xmax = min(xmin, xmax), max(xmin, xmax)\n            ymin, ymax = min(ymin, ymax), max(ymin, ymax)\n            bbox = [xmin / W, ymin / H, xmax / W, ymax / H]\n            bbox = (bbox * np.array([size, size, size, size])).astype(np.int16).tolist()\n            boxes.append(bbox)\n            labels.append(label)\n            \n        \n        # Defining \"target\"\n        target = {}\n        target['labels'] = torch.as_tensor([self.l2i[label] for label in labels])\n        target['boxes'] = torch.as_tensor(boxes)\n\n        return transformed_image, target\n\n    def collate_fn(self, batch):\n        return tuple(zip(*batch))\n\ndata_folder = \"/kaggle/input/cars-final-dataset/car\"\n\ndataset = DataSet(data_folder)\n\ntrain_size = int(0.7 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8,shuffle=True, collate_fn=dataset.collate_fn, drop_last=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True, collate_fn=dataset.collate_fn, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:48:42.533136Z","iopub.execute_input":"2023-12-05T23:48:42.533504Z","iopub.status.idle":"2023-12-05T23:48:43.247799Z","shell.execute_reply.started":"2023-12-05T23:48:42.533477Z","shell.execute_reply":"2023-12-05T23:48:43.247005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Draw batch function**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\nfrom torchvision.utils import draw_bounding_boxes\nimport numpy as np\nimport torch\n\ndef draw_batch_with_boxes(batch):\n    images, targets = batch\n    fig, axs = plt.subplots(2, 4, figsize=(20, 10)) # Adjust the size as needed\n    axs = axs.flatten()\n\n    for i, (image, target) in enumerate(zip(images, targets)):\n        # Convert image tensor to uint8 format for draw_bounding_boxes\n        image_uint8 = (image * 255).type(torch.uint8)\n\n        # Draw bounding boxes\n        boxes = target['boxes']\n        labels = [dataset.labels[l.item()] for l in target['labels']]\n        boxes_pil = draw_bounding_boxes(image_uint8, boxes, labels=labels, width=2, colors=\"blue\")\n\n        # Convert back to PIL to display\n        image_with_boxes = F.to_pil_image(boxes_pil)\n\n        # Plotting\n        axs[i].imshow(np.array(image_with_boxes))\n        axs[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage with a batch from the validation dataloader\nbatch = next(iter(val_dataloader))\ndraw_batch_with_boxes(batch)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:48:46.559017Z","iopub.execute_input":"2023-12-05T23:48:46.559406Z","iopub.status.idle":"2023-12-05T23:48:50.850951Z","shell.execute_reply.started":"2023-12-05T23:48:46.559375Z","shell.execute_reply":"2023-12-05T23:48:50.849765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**mAP metric**","metadata":{}},{"cell_type":"code","source":"# \"Helper\" function\ndef intersection_over_union(boxes_preds, boxes_labels):\n\n    box1_x1 = boxes_preds[..., 0:1]\n    box1_y1 = boxes_preds[..., 1:2]\n    box1_x2 = boxes_preds[..., 2:3]\n    box1_y2 = boxes_preds[..., 3:4]\n    box2_x1 = boxes_labels[..., 0:1]\n    box2_y1 = boxes_labels[..., 1:2]\n    box2_x2 = boxes_labels[..., 2:3]\n    box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection / (box1_area + box2_area - intersection + 1e-6)\n\n# mAP function\ndef mAP(predictions, ground_truths, iou_threshold=0.5, num_classes=2):\n    # predictions (list of lists) : [[idx_of_image, class, score, x1, y1, x2, y2], ...]\n    # ground_truths (list of lists) : [[idx_of_image, class, x1, y1, x2, y2], ...]\n\n\n    # Preprocessing data\n\n    # Reformatting predictions\n    reformatted_predictions = []\n    for image_number, prediction in enumerate(predictions):\n        boxes = prediction[\"boxes\"].cpu().detach().tolist()\n        labels = prediction[\"labels\"].cpu().detach().tolist()\n        scores = prediction[\"scores\"].cpu().detach().tolist()\n        for idx, box in enumerate(boxes):\n            add = [image_number, labels[idx], scores[idx], box[0], box[1], box[2], box[3]]\n            reformatted_predictions.append(add)\n\n    # Reformatting ground_truths\n    reformatted_ground_truths = []\n    for image_number, ground_truth in enumerate(list(ground_truths)):\n        boxes = ground_truth[\"boxes\"].tolist()\n        labels = ground_truth[\"labels\"].tolist()\n\n        for i in range(len(boxes)):\n            reformatted_prediction = [image_number, labels[i], boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]]\n            reformatted_ground_truths.append(reformatted_prediction)\n\n\n    ious = np.arange(0.5, 0.95, 0.05)\n    avg_precisions = []\n    eps = 1e-6\n    results = []\n\n    # Code of the funcion below\n    # 1) We need to group boxes with the same class\n    # 2) On the first step we need to group boxes with the same image index\n    for IOU in ious:\n        for c in range(num_classes):\n            detection_in_cur_class = []\n            ground_truths_in_cur_class = []\n\n            # Adding each one prediciton which has the 'c' class to handling (\"c\" in our case may be in {0,1,2,3} - num_classes)\n            for prediction in reformatted_predictions:\n                if prediction[1] == c:\n                    detection_in_cur_class.append(prediction)\n            # Adding each one gt box which has the 'c' class to handling\n            for ground_truth in reformatted_ground_truths:\n                if ground_truth[1] == c:\n                    ground_truths_in_cur_class.append(ground_truth)\n\n            # Dictionary, where key is an image index and value is a number of ground truth boxes there\n            boxes_per_image = Counter([ground_truth[0] for ground_truth in ground_truths_in_cur_class])\n            # Updating our dictionary, as we need to know if we covered some gt_boxes\n            for image_idx, num_gt_boxes in boxes_per_image.items():\n                boxes_per_image[image_idx] = torch.zeros(num_gt_boxes)\n\n            # Sorting predictions over scores in descending order\n            detection_in_cur_class.sort(key=lambda x: x[2], reverse=True)\n            # Initializing things through which we will get AP\n            TP = torch.zeros(len(detection_in_cur_class))\n            FP = torch.zeros(len(detection_in_cur_class))\n            Total_ground_truths = len(ground_truths_in_cur_class)\n\n\n            for detection_idx, detection in enumerate(detection_in_cur_class):\n                # GT boxes with the same class and image as \"detection\"\n                ground_truths_same_image = [box for box in ground_truths_in_cur_class if box[0] == detection[0]]\n                number_gts_for_particular_detection = len(ground_truths_same_image)\n\n                max_iou = 0\n                best_ground_truth_idx = 0\n\n                for idx, ground_truth in enumerate(ground_truths_same_image):\n                    iou = intersection_over_union(torch.tensor(detection[3:]), torch.tensor(ground_truth[2:]))\n\n                    if iou > max_iou:\n                        max_iou = iou\n                        best_ground_truth_idx = idx\n\n                if max_iou > IOU:\n                    if boxes_per_image[detection[0]][best_ground_truth_idx] == 0:\n                        TP[detection_idx] = 1\n                        boxes_per_image[detection[0]][best_ground_truth_idx] = 1\n                    else:\n                        FP[detection_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n            TP_cumsum = torch.cumsum(TP, dim=0)\n            FP_cumsum = torch.cumsum(FP, dim=0)\n            recalls = TP_cumsum / (Total_ground_truths + eps)\n            precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + eps))\n            precisions = torch.cat((torch.tensor([1]), precisions))\n            recalls = torch.cat((torch.tensor([0]), recalls))\n            avg_precisions.append(torch.trapz(precisions, recalls))\n        results.append(sum(avg_precisions) / len(avg_precisions))\n    return sum(results) / len(results)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:49:00.418822Z","iopub.execute_input":"2023-12-05T23:49:00.419816Z","iopub.status.idle":"2023-12-05T23:49:00.444162Z","shell.execute_reply.started":"2023-12-05T23:49:00.419774Z","shell.execute_reply":"2023-12-05T23:49:00.442723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model defining**","metadata":{}},{"cell_type":"code","source":"# Non pretrained\nmodel = torchvision.models.detection.ssd300_vgg16()\nnum_classes = 2\nanchors = model.anchor_generator.num_anchors_per_location()\nout_channels = [512, 1024, 512, 256, 256, 256]\nmodel.head.classification_head = SSDClassificationHead(out_channels, anchors, num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T23:03:24.342040Z","iopub.execute_input":"2023-12-04T23:03:24.342854Z","iopub.status.idle":"2023-12-04T23:03:26.474412Z","shell.execute_reply.started":"2023-12-04T23:03:24.342815Z","shell.execute_reply":"2023-12-04T23:03:26.473565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import SSD300_VGG16_Weights\n\n# Pretrained - CHOOSE IT\ndef create_model(num_classes=2):\n    # Load the Torchvision pretrained model.\n    model = torchvision.models.detection.ssd300_vgg16(\n        weights=SSD300_VGG16_Weights.COCO_V1\n    )\n    \n    num_classes = 2\n    anchors = model.anchor_generator.num_anchors_per_location()\n    out_channels = [512, 1024, 512, 256, 256, 256]\n    model.head.classification_head = SSDClassificationHead(out_channels, anchors, num_classes)\n    return model\n\nmodel = create_model(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:49:16.286110Z","iopub.execute_input":"2023-12-05T23:49:16.286764Z","iopub.status.idle":"2023-12-05T23:49:24.753334Z","shell.execute_reply.started":"2023-12-05T23:49:16.286729Z","shell.execute_reply":"2023-12-05T23:49:24.752409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train/Eval loop**","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU доступен. Используем GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU не доступен. Используем CPU.\")\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:49:25.963804Z","iopub.execute_input":"2023-12-05T23:49:25.964195Z","iopub.status.idle":"2023-12-05T23:49:28.983274Z","shell.execute_reply.started":"2023-12-05T23:49:25.964164Z","shell.execute_reply":"2023-12-05T23:49:28.982346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# optimizer = torch.optim.SGD(model.parameters(), lr=0.001,weight_decay=5e-4, momentum=0.9)\noptimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08) # - CHOOSE IT\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\nn_epochs = 13\n\nl_t_history = []\nl_v_history = []\nmAP_history = []\nbest_mAP_SSD = 0\nfor epoch in range(n_epochs):\n    loss_train_history = 0\n    loss_train_regression = 0\n    loss_train_classification = 0\n\n    model.train()\n    with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\") as pbar:\n        for i, data in enumerate(pbar):\n            images, targets = data\n            images = [image.to(device) for image in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            optimizer.zero_grad()\n            output = model(images, targets)\n            bbox_loss = output['bbox_regression']\n            classification_loss = output['classification']\n            losses = bbox_loss + classification_loss\n            losses.backward()\n            optimizer.step()\n\n            loss_train_history += losses.cpu().item()\n            loss_train_regression += bbox_loss.cpu().item()\n            loss_train_classification += classification_loss.cpu().item()\n\n        scheduler.step()\n        l_t_history.append(loss_train_history)\n        print(f\"Classifier training loss: {loss_train_classification / len(train_dataloader)}\")\n        print(f\"Box regression training loss: {loss_train_regression / len(train_dataloader)}\")\n\n\n    \nmodel.eval()\nmap_per_batch = []\nwith tqdm(val_dataloader, desc=f\"mAP evaluating\") as pbar:\n    with torch.no_grad():\n        for i, data in enumerate(pbar):\n            images, targets = data\n            images = [image.to(device) for image in images]\n\n            optimizer.zero_grad()\n            predictions = model(images)\n            map_this_batch = mAP(predictions, targets)\n            if map_this_batch > best_mAP_SSD:\n                best_mAP_SSD = map_this_batch\n            map_per_batch.append(map_this_batch)\n\n# Uncomment to mAP \n# mAP_history.append(sum(map_per_batch) / len(map_per_batch))\n# print(f\"Custom mAP on this epoch is: {sum(map_per_batch) / len(map_per_batch)}\")\n# print(\"Saving weights...\")\ntorch.save(model.state_dict(), 'model.pth') # Save our trained weights\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:36:48.883087Z","iopub.execute_input":"2023-12-05T06:36:48.883443Z","iopub.status.idle":"2023-12-05T06:37:50.068685Z","shell.execute_reply.started":"2023-12-05T06:36:48.883412Z","shell.execute_reply":"2023-12-05T06:37:50.067459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image printing**","metadata":{}},{"cell_type":"code","source":"i2l = { i: label for i, label in enumerate(['bg', 'car'])}\ndef decode_print(image, prediction):\n    # Preprocessing prediction\n\n    boxes = prediction[0][\"boxes\"].cpu().detach()\n    labels = np.array([i2l[i] for i in prediction[0]['labels'].cpu().detach().numpy()])\n    scores = prediction[0][\"scores\"].cpu().detach()\n\n    # Computing indexes with NonMaxSupression\n    indexes = nms(boxes, scores, 0.1).tolist()\n\n    boxes = boxes.tolist()\n    labels.tolist()\n    scores.tolist()\n\n    for index in indexes:\n        if (scores[index] > 0.2):\n            x1, y1, x2, y2 = list(map(int, boxes[index]))\n\n            w = abs(x2 - x1)\n            h = abs(y2 - y1)\n\n\n            text_size, _ = cv2.getTextSize(labels[index], cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)\n            text_origin = (x1, y1)\n\n            image = cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n            image = cv2.putText(image, labels[index] + \" \" + str(\"{:.3f}\".format(scores[index].item())), text_origin, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 255, 1)\n\n    plt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T23:49:39.994614Z","iopub.execute_input":"2023-12-05T23:49:39.995477Z","iopub.status.idle":"2023-12-05T23:49:40.007124Z","shell.execute_reply.started":"2023-12-05T23:49:39.995422Z","shell.execute_reply":"2023-12-05T23:49:40.005959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Weights + Inference**","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/baseline-weights/FirstBse.pth'))\n\nimg = Image.open(\"/kaggle/input/test-files/motorway.jpg\").convert(\"RGB\")\nimg_numpy = np.array(img.resize((720, 720), resample=Image.Resampling.BILINEAR)) / 255.\nimg = torch.from_numpy(img_numpy.astype('float32')).permute(2,0,1) # converting np_array, H,W,C -> torch_tensor, C,H,W\nimg = img[None, :]\n\nmodel.eval()\nimg = img.to(device)\nprediction = model(img)\n\ndecode_print(img_numpy, prediction)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T00:13:42.670765Z","iopub.execute_input":"2023-12-06T00:13:42.671177Z","iopub.status.idle":"2023-12-06T00:13:43.295237Z","shell.execute_reply.started":"2023-12-06T00:13:42.671145Z","shell.execute_reply":"2023-12-06T00:13:43.294228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Saving model to ONNX format**","metadata":{}},{"cell_type":"code","source":"model.eval()\n\ndummy_input = Image.open(\"/kaggle/input/test-files/motorway.jpg\").convert(\"RGB\")\nimg_numpy = np.array(dummy_input.resize((720, 720), resample=Image.Resampling.BILINEAR)) / 255.\ndummy_input = torch.from_numpy(img_numpy.astype('float32')).permute(2,0,1) # converting np_array, H,W,C -> torch_tensor, C,H,W\ndummy_input = dummy_input[None, :] # unsqueeze(0)\ndummy_input = dummy_input.to(device)\n\ntorch.onnx.export(model,\n                  dummy_input,\n                  \"/kaggle/working/onnx_model.onnx\", \n                  export_params=True,\n                  do_constant_folding=True,\n                  input_names = ['input'], \n                  output_names = ['output'])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T00:20:32.403580Z","iopub.execute_input":"2023-12-06T00:20:32.403999Z","iopub.status.idle":"2023-12-06T00:20:34.679166Z","shell.execute_reply.started":"2023-12-06T00:20:32.403966Z","shell.execute_reply":"2023-12-06T00:20:34.678123Z"},"trusted":true},"execution_count":null,"outputs":[]}]}